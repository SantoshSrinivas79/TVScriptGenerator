{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "level-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "from pickle import load\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "closing-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# load doc into memory\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, 'r')\n",
    "\ttext = file.read()\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "qualified-kennedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "                % While Jerry and Elaine are sitting on Jerry's couch watching \n",
      "                the tube, Elaine is flipping through the channels constantly.\n",
      "\n",
      "\n",
      "                Jerry: What are you doing? All right, all right. What's the matter \n",
      "                with that? What about that one?\n",
      "\n",
      "\n",
      "                Elaine: Robert Vaughn, The Helsinki Formula?\n",
      "\n",
      "\n",
      "                Jerry: He was good in Man From Uncle.\n",
      "\n",
      "\n",
      "                Elaine: Guess whose birthday's comin' up soon?\n",
      "\n",
      "\n",
      "                Jerry: I know, I'm having my root canal the same week.\n",
      "\n",
      "\n",
      "                Elaine: Oh, right. I hope you have a good oral surgeon because \n",
      "                that can be very serious. (Changes channel) Hey, look at naked \n",
      "                people.\n",
      "\n",
      "\n",
      "                Jerry: No, I don't wanna see the naked people.\n",
      "\n",
      "\n",
      "                Elaine: Been a while?\n",
      "\n",
      "\n",
      "                Jerry: I have a vague recollection of doing something with someone, \n",
      "                but it was a long, long time ago.\n",
      "\n",
      "\n",
      "                Elaine: I think my last time was in Rochester. My hair was a lot \n",
      "                shorter.\n",
      "\n",
      "\n",
      "                Jerry: I remember that it's a good thing. Someday, I hope to do \n",
      "                it again. (Jerry looks at Elaine)\n",
      "\n",
      "\n",
      "                Elaine: What?\n",
      "\n",
      "\n",
      "                Jerry: What?\n",
      "\n",
      "\n",
      "                Elaine: What was that look?\n",
      "\n",
      "\n",
      "                Jerry: What look?\n",
      "\n",
      "\n",
      "                Elaine: The look you just gave me.\n",
      "\n",
      "\n",
      "                Jerry: I gave a look?\n",
      "\n",
      "\n",
      "                Elaine: Yes.\n",
      "\n",
      "\n",
      "                Jerry: What kind of look?\n",
      "\n",
      "\n",
      "                Elaine: I know that look.\n",
      "\n",
      "\n",
      "                Jerry: Then what was it?\n",
      "\n",
      "\n",
      "                Elaine: Why should I tell you?\n",
      "\n",
      "\n",
      "                Jerry: Well, you're the big look expert. I wanna see how smart \n",
      "                you are.\n",
      "\n",
      "\n",
      "                Elaine: Trust me. I know the look. (Pause) So...\n",
      "\n",
      "\n",
      "                Jerry: What?\n",
      "\n",
      "\n",
      "                Elaine: What about the look?\n",
      "\n",
      "\n",
      "                Jerry: I don't know.\n",
      "\n",
      "\n",
      "                Elaine: You got some\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'cleaned_seinfeld_scripts.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pending-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\ttokens = doc.split()\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "awful-seattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['while', 'jerry', 'and', 'elaine', 'are', 'sitting', 'on', 'jerrys', 'couch', 'watching', 'the', 'tube', 'elaine', 'is', 'flipping', 'through', 'the', 'channels', 'constantly', 'jerry', 'what', 'are', 'you', 'doing', 'all', 'right', 'all', 'right', 'whats', 'the', 'matter', 'with', 'that', 'what', 'about', 'that', 'one', 'elaine', 'robert', 'vaughn', 'the', 'helsinki', 'formula', 'jerry', 'he', 'was', 'good', 'in', 'man', 'from', 'uncle', 'elaine', 'guess', 'whose', 'birthdays', 'comin', 'up', 'soon', 'jerry', 'i', 'know', 'im', 'having', 'my', 'root', 'canal', 'the', 'same', 'week', 'elaine', 'oh', 'right', 'i', 'hope', 'you', 'have', 'a', 'good', 'oral', 'surgeon', 'because', 'that', 'can', 'be', 'very', 'serious', 'changes', 'channel', 'hey', 'look', 'at', 'naked', 'people', 'jerry', 'no', 'i', 'dont', 'wanna', 'see', 'the', 'naked', 'people', 'elaine', 'been', 'a', 'while', 'jerry', 'i', 'have', 'a', 'vague', 'recollection', 'of', 'doing', 'something', 'with', 'someone', 'but', 'it', 'was', 'a', 'long', 'long', 'time', 'ago', 'elaine', 'i', 'think', 'my', 'last', 'time', 'was', 'in', 'rochester', 'my', 'hair', 'was', 'a', 'lot', 'shorter', 'jerry', 'i', 'remember', 'that', 'its', 'a', 'good', 'thing', 'someday', 'i', 'hope', 'to', 'do', 'it', 'again', 'jerry', 'looks', 'at', 'elaine', 'elaine', 'what', 'jerry', 'what', 'elaine', 'what', 'was', 'that', 'look', 'jerry', 'what', 'look', 'elaine', 'the', 'look', 'you', 'just', 'gave', 'me', 'jerry', 'i', 'gave', 'a', 'look', 'elaine', 'yes', 'jerry', 'what', 'kind', 'of', 'look', 'elaine', 'i', 'know', 'that', 'look', 'jerry', 'then', 'what', 'was', 'it']\n",
      "Total Tokens: 203898\n",
      "Unique Tokens: 12556\n"
     ]
    }
   ],
   "source": [
    "# looking at the clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "african-coordinator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 203847\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\tseq = tokens[i-length:i]\n",
    "\tline = ' '.join(seq)\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fallen-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "advisory-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'seinfeld_output.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "resident-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_filename = 'seinfeld_output.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incredible-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "thick-identifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "posted-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "italian-roommate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 50)            627850    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12557)             1268257   \n",
      "=================================================================\n",
      "Total params: 2,047,007\n",
      "Trainable params: 2,047,007\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aboriginal-colors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1593/1593 [==============================] - 66s 41ms/step - loss: 6.8289 - accuracy: 0.0370\n",
      "Epoch 2/10\n",
      "1593/1593 [==============================] - 67s 42ms/step - loss: 6.0801 - accuracy: 0.0657\n",
      "Epoch 3/10\n",
      "1593/1593 [==============================] - 68s 42ms/step - loss: 5.7516 - accuracy: 0.0911\n",
      "Epoch 4/10\n",
      "1593/1593 [==============================] - 66s 41ms/step - loss: 5.5558 - accuracy: 0.1026\n",
      "Epoch 5/10\n",
      "1593/1593 [==============================] - 67s 42ms/step - loss: 5.3624 - accuracy: 0.1122\n",
      "Epoch 6/10\n",
      "1593/1593 [==============================] - 67s 42ms/step - loss: 5.1791 - accuracy: 0.1239\n",
      "Epoch 7/10\n",
      "1593/1593 [==============================] - 65s 41ms/step - loss: 5.0477 - accuracy: 0.1302\n",
      "Epoch 8/10\n",
      "1593/1593 [==============================] - 65s 41ms/step - loss: 4.9114 - accuracy: 0.1351\n",
      "Epoch 9/10\n",
      "1593/1593 [==============================] - 66s 41ms/step - loss: 4.8028 - accuracy: 0.1393\n",
      "Epoch 10/10\n",
      "1593/1593 [==============================] - 67s 42ms/step - loss: 4.6905 - accuracy: 0.1461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x172ba3bb0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "chronic-customer",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-5de4d02b61bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# saving\n",
    "model.save('model.h5')\n",
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "homeless-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'cleaned_seinfeld_scripts.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "healthy-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "solar-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "heard-acting",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tokenizer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f8844af3a041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tokenizer.pkl'"
     ]
    }
   ],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"[Jerry and George are on the phone with Elaine] Jerry: I'll be right there. Elaine: What happened? George:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences([prompt])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, seq_length, prompt, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = prompt\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\tgen = model.predict_classes(encoded, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = generate(model, tokenizer, seq_length, prompt, 100)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mac_tensorflow",
   "language": "python",
   "name": "mac_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
